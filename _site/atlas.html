<!DOCTYPE html>
<html lang="en">

    <title>RNASeq | MLABST'17</title>

<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="keywords" content="uef, machine learning, speech, autonomous agents">


<!-- Google fonts -->
<link href='https://fonts.googleapis.com/css?family=Roboto:400,300' rel='stylesheet' type='text/css'>

<!-- Bootstrap -->
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/css/bootstrap.min.css">
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/css/bootstrap-theme.min.css">

<link rel="alternate" type="application/rss+xml" title="Machine Learning Applied to Speech Technology and Autonomous Agents RSS" href="/feed.xml" />
<link rel="stylesheet" href="http://localhost:4000/summerschool/css/main.css">






    <header class="site-header"  role="banner">

    <div class="navbar">
        <a id="navbar-title" href="http://localhost:4000/summerschool/">Machine Learning Applied to Speech Technology and Autonomous Agents</a><br />
        <a id="navbar-subtitle" href="http://www.uef.fi/en/web/summerschool/machine-learning-applied-to-speech-technology-and-autonomous-agents" target="_blank">UEF Summer School, 2018</a>
    </div>

</headerer>



<body>

    <main class="page-content" aria-label="Content">
        <div class="wrapper">
            <hr>
            <div class="container content">
                <h1 id="imaging-data---allen-brain-atlas">IMAGING DATA - Allen Brain Atlas</h1>

<h2 id="background-reading">Background reading</h2>
<p>http://www.nature.com/nbt/journal/v33/n5/abs/nbt.3209.html</p>

<h2 id="data">Data</h2>
<p>Allen Brain Atlas http://www.brain-map.org</p>

<h2 id="proposed-solution">Proposed solution</h2>

<h3 id="option-1">Option 1</h3>

<p>You could study this example approach proposed in mouse image study with CNN as a starting point: https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-015-0553-9 
and try to adapt this to human brain atlas, i.e. you would need to demonstrate that you can annotate gene expression patterns with human data.
To download images we have so far tested only one-by-one retrieval (you would need to find out if downloading all is even possible!), check instructions in:
http://help.brain-map.org/display/api/Downloading+an+Image
you need to find the desired imageIDs.</p>

<ul>
  <li>Do any search in http://human.brain-map.org/ish/search</li>
  <li>Click a SpecimenID in the search results</li>
  <li>Click the experiment ID in the “Selection Information”-box</li>
  <li>There is a small button in the top right corner of the image. Howering the mouse over it displays a text “Launch a high resolution viewer in a new window”. Click it.</li>
  <li>You can either download the images here using the download button or asinstructed in:
http://help.brain-map.org/display/api/Downloading+an+Image
and download the image using the imageId shown in the web address of this window.</li>
  <li>For every ISH-image there is an image showing gene expression areas and a Nissil-reference image of the shown area. Those images can also be viewed in this window.
Properties of search interface are explained here  http://human.brain-map.org/ish/search</li>
</ul>

<p>The next step (which you can complete even w/o image data) is to critically evaluate the example approach proposed in mouse image study with CNN: https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-015-0553-9 
To get it up and running for doing some tests, here some hints:</p>
<ul>
  <li>VLFeat can be used via Matlab: http://www.vlfeat.org/install-matlab.html</li>
  <li>SIFT-algorithm to extract feature vectors. User guide: http://www.vlfeat.org/overview/sift.html</li>
  <li>Bag-of-words model generated from the image using random sampling + obtained feature vectors -&gt;  K-means-algorithm -&gt; visual codebook</li>
  <li>The OverFeat-neural network structure and source code is available in github ( https://github.com/sermanet/OverFeat ). It comes with python and torch APIs.</li>
  <li>The model comes with pre-trained weights.</li>
  <li>Example of the usage ( https://github.com/sermanet/OverFeat/blob/master/API/python/sample.py )</li>
  <li>The OverFeat-model’s input need to be resized or cropped to 231x231 and when using python it needs to be given to the model in a numpy array with dtype=numpy.float32 in the shape of 3xHxW (RGB image with size HxW). Images with the size HxWx3 can be transposed to desired shape.</li>
</ul>

<p>Option 2
Implement your own network model, motivated by earlier literature and what you learned so far. Here you would also need to describe how to obtain training and test data (but if your focus is on method development, we will not require so much time spent getting the data, unless you aim for the leader board where we will give higher rank to full solutions)</p>
<ul>
  <li>For gene localization check above instructions for Specimen section’s expression images (ground truth)</li>
  <li>Implement network e.g. in python using keras with tensorflow. You can make something like the OverFeat model’s architechture (Figure 2 in https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-015-0553-9 ) or implement your own.</li>
  <li>Possible tricky parts: shape of tissue sections varies and also location where section was cut from. You might need to normalize your data somehow. You should propose some solutions here.</li>
  <li>Compared to Nissil-reference images some of the Allen Brain Atlas images are very dim -&gt; you may need to smooth the color data. Again, propose some solutions here.</li>
</ul>

<p>To get a good rank on the leaderboard, you should benchmark your solution with simulations and/or real data.</p>

<h3 id="evaluation">Evaluation</h3>

<p>clever choice of methodology
clever use of data
demonstrating preliminary success, minimum requirement is that your approach is able to detect neuronal loss in AD</p>

            </div>
        </div>
    </main> 
    
    <footer class="site-footer">
  <div class="footer"></div>
</footer>

        
</body>
